---
title: "p8105_hw3_db3180"
author: "Divya Bisht"
date: "10/9/2018"
output: github_document
---

## Problem 1 
Uploading problem 1 dataset
```{r setup, include=FALSE}
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
brfss_data = brfss_smart2010 %>%
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  rename(state = locationabbr, county = locationdesc) %>%
  janitor::clean_names() %>% 
  mutate(response = factor(response, levels = c("Excellent",  "Very good", "Good", "Fair", "Poor")))
```

```{r}
brfss_2002 = brfss_data %>% 
  janitor::clean_names() %>%
  filter(year == 2002) %>%
  group_by(state) %>% 
  distinct(state, county) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

CT, FL, and NC were the states that were observed at 7 locations. 
```{r}
spaghetti_brfss = brfss_data %>%
  group_by(state, year) %>% 
  summarize(locations = n()) %>% 
    
    ggplot(aes(x = year, y = locations, color = state)) + 
    geom_line()
```
Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_data %>%
  filter(year == 2002 | year == 2006 | year == 2010, state == "NY") %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
             sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable()
```

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss_data %>%
  spread(response, data_value) %>% 
  group_by(state, year) %>% 
  janitor::clean_names() %>%
  summarize(mean_excellent = mean(excellent, na.rm = TRUE), 
            mean_verygood = mean(very_good, na.rm = TRUE), 
            mean_good = mean(good, na.rm = TRUE), 
            mean_fair = mean(fair, na.rm = TRUE), 
            mean_poor = mean(poor, na.rm = TRUE)) %>%
  gather(response, data_values, mean_excellent, mean_verygood, mean_good, mean_fair, mean_poor) %>% 
  
  
  ggplot(aes(x = year, y = data_values, color = state)) +
  geom_line() + 
  facet_grid(~response)
```


##Problem 2
```{r}
instacart_data = instacart %>%
  janitor::clean_names()
```
This dataset has 1384617 rows/observations and 15 columns/variables. Some key variables include: product_id, reordered (which tells us if the product had been reordered by a customer), order_hour_of_day, aisle, department, etc. 



```{r}
aisle = nrow(distinct(instacart_data, aisle))
```
There are `r aisle` aisles. 
```{r}
aisle_most = instacart_data %>% 
  group_by(aisle_id, aisle) %>%
  summarize(n = n()) %>% 
  arrange(desc(n))
```
The aisle that most of the products are ordered from are the fresh vegetables, fresh fruits and packaged vegetable fruits. 

##Problem 3
```{r}
ny_noaa_data = ny_noaa %>% 
  janitor::clean_names()
```
This data has 2595176 rows/observations and 7 columns/variables. Some key variables are: id(weather stattion ID, date (date of observation), prcp (Precipitation (tenths of mm), snow (Snowfall (mm)), and tmax and tmin (max and minimum temperatures). 

