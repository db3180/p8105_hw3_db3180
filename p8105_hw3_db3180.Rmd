---
title: "p8105_hw3_db3180"
author: "Divya Bisht"
date: "10/9/2018"
output: github_document
---

## Problem 1 
Uploading problem 1 dataset
```{r setup, include=FALSE}
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
brfss_data = brfss_smart2010 %>%
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  rename(state = locationabbr, county = locationdesc) %>%
  janitor::clean_names() %>% 
  mutate(response = factor(response, levels = c("Excellent",  "Very good", "Good", "Fair", "Poor")))
```

```{r}
brfss_data %>% 
  janitor::clean_names() %>%
  filter(year == 2002) %>%
  group_by(state) %>% 
  distinct(state, county) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

CT, FL, and NC were the states that were observed at 7 locations. 
```{r}
brfss_data %>%
  group_by(state, year) %>% 
  summarize(locations = n()) %>% 
    
    ggplot(aes(x = year, y = locations, color = state)) + 
    geom_line()
```
Table showing: for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_data %>%
  filter(year == 2002 | year == 2006 | year == 2010, state == "NY") %>% 
  spread(response, data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state, year) %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
             sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable()
```

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss_data %>%
  spread(response, data_value) %>% 
  group_by(state, year) %>% 
  janitor::clean_names() %>%
  summarize(mean_excellent = mean(excellent, na.rm = TRUE), 
            mean_verygood = mean(very_good, na.rm = TRUE), 
            mean_good = mean(good, na.rm = TRUE), 
            mean_fair = mean(fair, na.rm = TRUE), 
            mean_poor = mean(poor, na.rm = TRUE)) %>%
  gather(response, data_values, mean_excellent, mean_verygood, mean_good, mean_fair, mean_poor) %>% 
  
  ggplot(aes(x = year, y = data_values, color = state)) +
  geom_line() + 
  facet_grid(~response)
```


## Problem 2
```{r}
instacart_data = instacart %>%
  janitor::clean_names()
```
This dataset has 1384617 rows/observations and 15 columns/variables. Some key variables include: product_id, reordered (which tells us if the product had been reordered by a customer), order_hour_of_day, aisle, department, etc. 



```{r}
aisle = nrow(distinct(instacart_data, aisle))
```
There are `r aisle` aisles. 
```{r}
instacart_data %>% 
  group_by(aisle_id, aisle) %>%
  summarize(n = n()) %>% 
  arrange(desc(n))
```
The aisle that most of the products are ordered from are the fresh vegetables, fresh fruits and packaged vegetable fruits. 


Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
instacart_data %>% 
  group_by(aisle) %>%
  summarize(n_items = n()) %>% 
  arrange(desc(n_items)) %>% 

ggplot(aes(x = aisle, y = n_items)) +
  geom_point()
```

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
instacart_data %>% 
  janitor::clean_names() %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" 
                        | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  top_n(1) %>%
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
instacart_data %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(order_dow, mean_hour)
```


##Problem 3
```{r}
ny_noaa_data = ny_noaa %>% 
  janitor::clean_names()
```
This data has 2595176 rows/observations and 7 columns/variables. Some key variables are: id(weather stattion ID, date (date of observation), prcp (Precipitation (tenths of mm), snow (Snowfall (mm)), and tmax and tmin (max and minimum temperatures). 

